{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"entropyhub_flame/","text":"\u00b6 Entropy creates and manages workflows in a clean, modular manner, integrating experimental hardware and computational resources. It ensures hardware performance using natively distributed event-driven state-machine execution, while user productivity is boosted with industry leading web-based interfaces. Entropy is a modular, open-source solution that can be easily adopted and expanded for orchestration of the most challenging experiments, instruments and quantum processors. Top level interface is provided by Entropy Hub applicatoin that allows every step of control and execution to be authored, monitored, managed and interrogated from the web interface, from any web browser on any device on the shared local network. At the core of the sustem, allowing powerful and scallable execution is Entropy Flame engine. Overview \u00b6 Modern experiments and instruments, and in particular quantum computing platforms, require integration of many software and hardware resources, in a manner that will allow easy creation, maintenance, and scaling. Entropy abstracts away architecture issues, and provides a simple framework where users can focus on solving problems they know best. This is achieved by Node I/O library , that wraps individual modules that users can develop in their language of choice, providing a universal interface. It allows building and testing the modules independently of the rest of the system, and seamless high-performance integration of the module once it is run as a part of a bigger workflow, without changing anything in the user code. It also provides a very expressive way of describing events, data flows and states on which execution of such a module will depend. Workflow library is a fluent, expressive way to describe workflow of your experiment with very little Python code, very close to everyday language, with a dynamic documentation always at-hand for all your nodes. With this you can easily orchestrate the whole experiment, using multiple programming languages, platforms and heterogenous hardware. Execution is taken care by the Entropy Flame, which will enable you also to easily find and debug any problems that might occur during R&D of your experiment, to track real-time operation, record notes and data, and easily find all information across the system. Overview of Entropylab: NodeIO library provides universal interface to the code, and allows testing of individual modules. Individually tested nodes can be immersed in workflow, that is defined as a Python script, that defines interaction between the node actors. Key features \u00b6 Natively distributed execution, connects heterogenous asynchronous systems: this is scalable solution with no bottlenecks Low-latency between nodes, bare-metal access Development of complex solutions module by module, and good observability of things that work as well as those that don't work and need debugging. Expressive user interface allows describing solution as a story narrative Management interface provided: jobs, notes and results are connected, persisted and versioned, accessible through enterprise-quality search. Example use-case \u00b6 To-do: smart home IoT","title":"Overview"},{"location":"entropyhub_flame/#_1","text":"Entropy creates and manages workflows in a clean, modular manner, integrating experimental hardware and computational resources. It ensures hardware performance using natively distributed event-driven state-machine execution, while user productivity is boosted with industry leading web-based interfaces. Entropy is a modular, open-source solution that can be easily adopted and expanded for orchestration of the most challenging experiments, instruments and quantum processors. Top level interface is provided by Entropy Hub applicatoin that allows every step of control and execution to be authored, monitored, managed and interrogated from the web interface, from any web browser on any device on the shared local network. At the core of the sustem, allowing powerful and scallable execution is Entropy Flame engine.","title":""},{"location":"entropyhub_flame/#overview","text":"Modern experiments and instruments, and in particular quantum computing platforms, require integration of many software and hardware resources, in a manner that will allow easy creation, maintenance, and scaling. Entropy abstracts away architecture issues, and provides a simple framework where users can focus on solving problems they know best. This is achieved by Node I/O library , that wraps individual modules that users can develop in their language of choice, providing a universal interface. It allows building and testing the modules independently of the rest of the system, and seamless high-performance integration of the module once it is run as a part of a bigger workflow, without changing anything in the user code. It also provides a very expressive way of describing events, data flows and states on which execution of such a module will depend. Workflow library is a fluent, expressive way to describe workflow of your experiment with very little Python code, very close to everyday language, with a dynamic documentation always at-hand for all your nodes. With this you can easily orchestrate the whole experiment, using multiple programming languages, platforms and heterogenous hardware. Execution is taken care by the Entropy Flame, which will enable you also to easily find and debug any problems that might occur during R&D of your experiment, to track real-time operation, record notes and data, and easily find all information across the system. Overview of Entropylab: NodeIO library provides universal interface to the code, and allows testing of individual modules. Individually tested nodes can be immersed in workflow, that is defined as a Python script, that defines interaction between the node actors.","title":"Overview"},{"location":"entropyhub_flame/#key-features","text":"Natively distributed execution, connects heterogenous asynchronous systems: this is scalable solution with no bottlenecks Low-latency between nodes, bare-metal access Development of complex solutions module by module, and good observability of things that work as well as those that don't work and need debugging. Expressive user interface allows describing solution as a story narrative Management interface provided: jobs, notes and results are connected, persisted and versioned, accessible through enterprise-quality search.","title":"Key features"},{"location":"entropyhub_flame/#example-use-case","text":"To-do: smart home IoT","title":"Example use-case"},{"location":"entropyhub_flame/getting_started/","text":"Getting started \u00b6 Entropy has a web-browser based interface, and includes the whole stack you need for writing, executing, managing and storing your experimental workflows that are setup in default installation with a few commands. Advanced users on the other hand might be interested in advanced modular installations that are also supported. Simple installation \u00b6 This is for method that installs all components of EntropyHub on a single-machine. Choose installation depending if you are using EntropyHub or developing it: User of EntropyHab and Flame Developer/Contributor In the folder where you want to setup entropyhub and project files do from terminal pip install entropyhub python -m entropyhub git clone --depth 1 https://github.com/entropy-lab/entropy-hub.git cd entropy-hub pip install --upgrade --force-reinstall ./entropylab ./entropyhub python -m entropyhub --build When starting the first time, this will take some time since the system is being downloaded and initialised. Once the terminal information stops changing, open http://localhost in your web-browser. To switch off entropy press Ctrl + C in terminal once, and wait for the system to switch off. To run again EntropyHub, it is enough to run python -m entropyhub in the same folder again. Running an example \u00b6 Open in the web browser http://localhost . Open Experiments panel. Code editor will open. Left-Click on main_pc on the side menu and select Open remote SSH terminal to open terminal. In terminal window type python3 -m entropylab.flame.example.simple And follow up on commands there. Alternatively, you can try more complex example by running python3 -m entropylab.flame.example.complex For final workflow execution, switch to workflow graphical view and press Run . To start empty project start python3 -m entropylab.flame.template.empty","title":"Getting started"},{"location":"entropyhub_flame/getting_started/#getting-started","text":"Entropy has a web-browser based interface, and includes the whole stack you need for writing, executing, managing and storing your experimental workflows that are setup in default installation with a few commands. Advanced users on the other hand might be interested in advanced modular installations that are also supported.","title":"Getting started"},{"location":"entropyhub_flame/getting_started/#simple-installation","text":"This is for method that installs all components of EntropyHub on a single-machine. Choose installation depending if you are using EntropyHub or developing it: User of EntropyHab and Flame Developer/Contributor In the folder where you want to setup entropyhub and project files do from terminal pip install entropyhub python -m entropyhub git clone --depth 1 https://github.com/entropy-lab/entropy-hub.git cd entropy-hub pip install --upgrade --force-reinstall ./entropylab ./entropyhub python -m entropyhub --build When starting the first time, this will take some time since the system is being downloaded and initialised. Once the terminal information stops changing, open http://localhost in your web-browser. To switch off entropy press Ctrl + C in terminal once, and wait for the system to switch off. To run again EntropyHub, it is enough to run python -m entropyhub in the same folder again.","title":"Simple installation"},{"location":"entropyhub_flame/getting_started/#running-an-example","text":"Open in the web browser http://localhost . Open Experiments panel. Code editor will open. Left-Click on main_pc on the side menu and select Open remote SSH terminal to open terminal. In terminal window type python3 -m entropylab.flame.example.simple And follow up on commands there. Alternatively, you can try more complex example by running python3 -m entropylab.flame.example.complex For final workflow execution, switch to workflow graphical view and press Run . To start empty project start python3 -m entropylab.flame.template.empty","title":"Running an example"},{"location":"entropyhub_flame/contribute/overview/","text":"Entropy in computer science language \u00b6 End-user documentation is for users of Entropy - i.e. scientists. It aims to explain capabilities in general language, staying away from domain specific jargon. All distributed computing language should be abstracted away and interface should be made for general scientists, without requiring them to learn a lengthy glossary of terms. However, since you are now looking at a guide for (software) contributors, we will be using all computer science terminology to ease developer onboarding. Entropy is actor framework for distributed computing Importantly, it is not just data-pipeline workflow (although you can write one if you want), or asynchronous framework (although you can use it as such). Instead all actors run in parallel, in so much as the operating system and a number of cores on underlying hardware enables real parallelism, and does not do just asynchronous process interleaving. can be seen as sort of distributed operating system from one perspective (one workflow runs on heterogeneous hardware), that acts as virtualisation level on top of the existing operating system (check Plan9 and Inferno) actors use asynchronous, broker-less communication done with ZeroMQ (0MQ) library, with MessagePack used for schema-less communication serialisation. ZeroMQ is broker-less communication that allows for all sorts of communication topologies (not only client-server), and allows use of different underlying communication mechanisms (in process shared memory, inter-process via sockets/pipes/TCP, broadcasts via UDP and more). Entropy aims to be language-agnostic in terms of the language for writing actors. NodeIO library uses just ZeroMQ and MessagePack that have implementations for a larga number of programming languages Entropy does not do isolation of individual processes and this is by design: since these processes are controlling experimental hardware, that is other devices connected to the machine, we need to provide full bare-metal access to the machine. Containers and isolation can be built on top of the Entropy framework if needed, e.g. by running some nodes as containers. Lack of isolation also allows easy in-memory passing of large data sets within the same machine to be done by users if needed for advanced use. Now in addition to authoring the nodes and workflows, you might be interested in technical details of the workflow execution. Entropy executor (Flame) starts individual nodes from the command line, passing required information about the workflow as two command-line arguments. This works on all types of hardwares and OS, and should","title":"Guide for contributors"},{"location":"entropyhub_flame/contribute/overview/#entropy-in-computer-science-language","text":"End-user documentation is for users of Entropy - i.e. scientists. It aims to explain capabilities in general language, staying away from domain specific jargon. All distributed computing language should be abstracted away and interface should be made for general scientists, without requiring them to learn a lengthy glossary of terms. However, since you are now looking at a guide for (software) contributors, we will be using all computer science terminology to ease developer onboarding. Entropy is actor framework for distributed computing Importantly, it is not just data-pipeline workflow (although you can write one if you want), or asynchronous framework (although you can use it as such). Instead all actors run in parallel, in so much as the operating system and a number of cores on underlying hardware enables real parallelism, and does not do just asynchronous process interleaving. can be seen as sort of distributed operating system from one perspective (one workflow runs on heterogeneous hardware), that acts as virtualisation level on top of the existing operating system (check Plan9 and Inferno) actors use asynchronous, broker-less communication done with ZeroMQ (0MQ) library, with MessagePack used for schema-less communication serialisation. ZeroMQ is broker-less communication that allows for all sorts of communication topologies (not only client-server), and allows use of different underlying communication mechanisms (in process shared memory, inter-process via sockets/pipes/TCP, broadcasts via UDP and more). Entropy aims to be language-agnostic in terms of the language for writing actors. NodeIO library uses just ZeroMQ and MessagePack that have implementations for a larga number of programming languages Entropy does not do isolation of individual processes and this is by design: since these processes are controlling experimental hardware, that is other devices connected to the machine, we need to provide full bare-metal access to the machine. Containers and isolation can be built on top of the Entropy framework if needed, e.g. by running some nodes as containers. Lack of isolation also allows easy in-memory passing of large data sets within the same machine to be done by users if needed for advanced use. Now in addition to authoring the nodes and workflows, you might be interested in technical details of the workflow execution. Entropy executor (Flame) starts individual nodes from the command line, passing required information about the workflow as two command-line arguments. This works on all types of hardwares and OS, and should","title":"Entropy in computer science language"},{"location":"entropyhub_flame/management_record/overview/","text":"Management of records \u00b6 Components \u00b6 Job submission, tracking and versioning \u00b6 Raw data explorer \u00b6 Once the job is finished, runtime saves all data (node outputs with retention period set to 2) into a single HDF5 file per job, together with all metadata about the job. These files can be seen in Data panel of the Entropy Hub. Select from the list experiment run, and use HDF5 Explorer to explore the content of the file. Each file root, node and data saves metadata information about job, that node, and output descriptions respectively. These can be seen by selecting Inspect switch. Note that HDF5 Explorer can visualise only native HDF5 data, which are N-dimensional arrays. To save such from Python NodeIO output you can use output . set ( some_output_value = 2 ) output . set ( some_output_1D_array = [ 1 , 2 , 3 ]) output . set ( some_output_2D_array = [[ 1 , 2 ],[ 3 , 4 ]]) Setting output multiple times will generate an additional 1D array in the HDF5 output. Timestamps when the data records are written are saved in <value name>_time variables Any other JSON compatible JSON dictionary will be saved as string, and they can be accessed directly programmatically for use and analysis in other programs - see Accessing data when entropy hub is not running below Looking at data with user-defined views \u00b6 Notebook \u00b6 Data export & backup \u00b6 See the follwing section. Accessing data when entropy hub is not running \u00b6 All data is stored in the data_store folder. Experimental data, together with all relevant metadata about the workflow and job that produced the data, is stored in data_store/<project prefix>/<jobid>.hdf5 , with one file corresponding to one job (one experimental run). Some additional information is saved for debugging purposes: standard output and standard error output of all the nodes participating in the workflow are saved for each job under data_store/project prefix>/<job id>/<node name>.log . Finally workflow files and parameter files are saved in corresponding git repositories data_store/git/<project prefix>/workflow.git and data_store/git/<project prefix>/params.git respectively. HDF5 data files as attributes of the root contain information on the job, including corresponding git SHA1 commit tags for workflow and parameters that can be used for manual extraction of corresponding run, when Entropy Hub is not running (when Entropy Hub is running, this is all automatically done from the web GUI).","title":"Management of records"},{"location":"entropyhub_flame/management_record/overview/#management-of-records","text":"","title":"Management of records"},{"location":"entropyhub_flame/management_record/overview/#components","text":"","title":"Components"},{"location":"entropyhub_flame/management_record/overview/#job-submission-tracking-and-versioning","text":"","title":"Job submission, tracking and versioning"},{"location":"entropyhub_flame/management_record/overview/#raw-data-explorer","text":"Once the job is finished, runtime saves all data (node outputs with retention period set to 2) into a single HDF5 file per job, together with all metadata about the job. These files can be seen in Data panel of the Entropy Hub. Select from the list experiment run, and use HDF5 Explorer to explore the content of the file. Each file root, node and data saves metadata information about job, that node, and output descriptions respectively. These can be seen by selecting Inspect switch. Note that HDF5 Explorer can visualise only native HDF5 data, which are N-dimensional arrays. To save such from Python NodeIO output you can use output . set ( some_output_value = 2 ) output . set ( some_output_1D_array = [ 1 , 2 , 3 ]) output . set ( some_output_2D_array = [[ 1 , 2 ],[ 3 , 4 ]]) Setting output multiple times will generate an additional 1D array in the HDF5 output. Timestamps when the data records are written are saved in <value name>_time variables Any other JSON compatible JSON dictionary will be saved as string, and they can be accessed directly programmatically for use and analysis in other programs - see Accessing data when entropy hub is not running below","title":"Raw data explorer"},{"location":"entropyhub_flame/management_record/overview/#looking-at-data-with-user-defined-views","text":"","title":"Looking at data with user-defined views"},{"location":"entropyhub_flame/management_record/overview/#notebook","text":"","title":"Notebook"},{"location":"entropyhub_flame/management_record/overview/#data-export-backup","text":"See the follwing section.","title":"Data export &amp; backup"},{"location":"entropyhub_flame/management_record/overview/#accessing-data-when-entropy-hub-is-not-running","text":"All data is stored in the data_store folder. Experimental data, together with all relevant metadata about the workflow and job that produced the data, is stored in data_store/<project prefix>/<jobid>.hdf5 , with one file corresponding to one job (one experimental run). Some additional information is saved for debugging purposes: standard output and standard error output of all the nodes participating in the workflow are saved for each job under data_store/project prefix>/<job id>/<node name>.log . Finally workflow files and parameter files are saved in corresponding git repositories data_store/git/<project prefix>/workflow.git and data_store/git/<project prefix>/params.git respectively. HDF5 data files as attributes of the root contain information on the job, including corresponding git SHA1 commit tags for workflow and parameters that can be used for manual extraction of corresponding run, when Entropy Hub is not running (when Entropy Hub is running, this is all automatically done from the web GUI).","title":"Accessing data when entropy hub is not running"},{"location":"entropyhub_flame/nodeio/overview/","text":"NodeIO \u00b6 NodeIO wraps your code \u2014 that provides a solution using some experimental hardware or does some calculations \u2014 written in almost any language. We call that solution a node , and NodeIO provides a universal interface to your solution. Taking NodeIO in the flavour of your language of choice, you can individually test your solution (modularity) before immersing it in a bigger workflow , for which NodeIO provided an interface without any additional work from your side. Crucially, through inputs and outputs provided by NodeIO your solution can trigger, pass and react to events, data and states in other nodes in the workflow. Thanks to NodeIO you explain all of these in a way that closely follows how we humans think, as a story that happens. Note When using EntropyHub, code for nodes and workflows can be edited in Experiments panel with Code view turned on. Example of node definition \u00b6 Before looking in details, let's see an example: Python QUA C++ LabView # ==================== DEFINE NODE ==================== import entropylab.flame.nodeio as nodeio nodeio . context ( name = \"HelloWorld\" , description = \"example node that concatinates strings\" , icon = \"bootstrap/gear-fill.svg\" ) inputs = nodeio . Inputs () inputs . flow ( \"new_value\" , units = \"\" , description = \"what to add to cummulative sum\" ) outputs = nodeio . Outputs () outputs . define ( \"cumulative_sum\" , units = \"\" , description = \"string after concatination\" , retention = 0 ) nodeio . register () # ==================== DRY RUN DATA ==================== inputs . set ( new_value = \"Hello\" ) inputs . set ( new_value = \" world\" ) inputs . set ( new_value = \"!\" ) # =============== RUN NODE STATE MACHINE =============== current_string = \"\" while nodeio . status . active : outputs . set ( cumulative_sum = current_string ) print ( current_string ) increment = inputs . get ( \"new_value\" ) current_string += increment # TO-DO #include <stdio.h> int main ( void ) { printf ( \"TO-DO! \\n \" ); return 0 ; } TO-DO We see the code has in general three parts: 1. node and I/O definition, 2. dry-run data and 3. user-defined logic that works within a state-machine loop. Looking closely to example you see that: First we use the nodeio library to define node name , used for creating instances of this node when building workflows later. Also, we specify description of the node, and select an icon for visualisation in the workflow view. After that we grab an instance of nodeio.Inputs() and use it to define names of inputs , specifying optional units and some useful description. Here we use the so-called flow variable that we will discuss below. Think about them as a water pipe, they queue all received inputs for the node to be consumed in sequential manner. Then we grab an instance of nodeio.Outputs() and use it to define names of outputs . Additional key-word argument retention will specify now if we need this output just temporarily, as run-time variable only, or we also want to save it forever, as a result. Importantly, at the end of this segment we call nodeio.register() which will register this node as something we can use later on in building workflows. The second segment defines so-called dry-run data . This is used for specifying example inputs of the node, and allows testing and debugging of the node during development of the node, independent of any workflows. It is also used to specify expected data-types for the inputs, which in case of Python is done implicitly. In this specific example, we set the flow variable 3 times. Since this is a flow variable, the input.get function called later for this variable will return these three different values in sequence they were passed to it. Finally, in the third, and the last part, we have user-defined logic for solving specific problems. Importantly, after any initial variable initialisation, all the logic is happening within state-machine loop defined by nodeio.status.active . One breaks out of the loop pressing Ctrl + C , or the program will on its own leave the loop if some of the requested flow variables is empty. During the running, execution relative to other nodes is controlled by setting outputs and getting inputs. For example, If a node is run as a part of a workflow, input.get() will block on flow variables until value is received from some other node. If we run above code in terminal the output will be Hello Hello world Hello world! When running node like this, outside a workflow (dry-run), not only that we check the functioning of the node, but also on the background the necessary node information is added in the automatically-generated folder entropynodes that can be used later on to construct workflows. What data types we can pass as inputs/outputs? To keep this interoperability between different programing languages, node inputs and outputs can be a int , float , string or generic json . JSON can be used to pass any other datatype by serializing it beforehand. We will now look in detail at the options and functionality of nodio inputs and outputs. Node inputs \u00b6 Communication between nodes is asynchronous, and different types of input variables handle differently updates to the inputs. Choosing appropriate input can help you easily write solutions for real-time control with many nodes. With each node input type we provide some typical use cases to help you with selection. State \u00b6 A state input saves only the last value passed to it. Requesting multiple times the value of state input always returns the latest existing value, no metter how many times the new value has been set since the last get(...) call. The get(...) call blocks (waits) for value only if the value has not yet been set. A state input It is defined with Python C++ inputs . state ( \"temperature\" , units = \"C\" , description = \"sensor reading of outside temperature\" ) \\\\ TO - DO Value can be obtained with Python C++ value = inputs . get ( \"temperature\" ) \\\\ TO - DO And this request will return the latest set value, unless the value has not been set yet, in which case it will block (wait) until it is set. We can use method updated(...) to check if the value has been updated since the last time we have read it Python C++ if inputs . updated ( \"temperature\" ): print ( \"temperature value has been set again on input\" ) else : print ( \"no updates received\" ) \\\\ TO - DO This always returns immediately. Note that updated returns True even if the same value has been set again (it does not compare with previous value). Some use cases are Providing latest real-time parameters Pipelines for real-time data Imagine you have some process that continuously does analysis of available data about your system and updates some value estimate. The value might be describing best current treshold for discriminating between two states for example. Some other part of your system has to use this knowledge. That part of the system does not care how many updates have been made to the parameter in the meantime by . All wants to do is to use the latest/best available value, while working independently of the . Connecting output as state input for is a perfect solution. Imagine you have a camera taking images of something at 30 fps. This is images are used for several different purposes in the system. Now you want to add to this a special image processing module . However due to heavy processing, your can handle just 20 fps on average. To make best use of the real-time system, you would like to feed to the rest of the system the most up-to-date analysis from the module, so you might need to periodically drop some of the frames. For this connecting output to state input of is a perfect solution. Flow \u00b6 A flow input queues all received values for sequential consumption one by one. Each .get(...) then returns (consumes) one value from the queue. In case the queue is empty, get(...) queue will block (wait). Python C++ inputs . flow ( \"movement_detected\" , units = \"my units for movement detection\" , description = \"details of detected movement\" ) \\\\ TO - DO Value can be obtained same as with any other input Python C++ value = inputs . get ( \"movement_detected\" ) \\\\ TO - DO If the queue is empty this will block (wait) until something sets the input. Repeated calls to get(...) on flow variables return in sequence items from the queue in the order they are received (queue is consumed in order). Sometimes one might just want to check if the new value on the flow input is provided, without blocking the get(...) call. That is provided by updated(...) method Python C++ if inputs . updated ( \"movement_detected\" ): print ( \"New movement has been detected. Details of the movement:\" ) details = inputs . get ( \"movement_detected\" ) print ( details ) else : print ( \"no movement detected\" ) \\\\ TO - DO This always returns immediately, returning True only if there is a value in the flow input queue ready to be consumed with get(...) . Some use cases are Event triggering Data pipeline/queue Imagine you want to trigger e.g. camera on movement . You can solve this by having a camera node that will initialize a camera and wait on flow input channel, connected to node that does motion detector analysis maybe based on some sensors, to trigger an event . Imagine you have a chain of data analysis nodes ( , , , , ...) operating in sequence on a given data. You can pass these data tasks from one node to another using flow inputs. . Note in case that you data producers are faster than data consumers, the data buffer will be growing, occupying memory. If data production is not naturally limited within the system, unlimited amounts of data can be produced over time, while the consumer is limited in ingestion. Then the buffer would request ever-growing memory in this case. This unlimited memory cannot be provided of course. To handle such cases please check the note below. Note: what to do when producer is faster than consumer Note that simple flow input does not provide back-pressure on producers. That is, if producer is producing output at rate larger than the input node consumption on flow input, the data will be saved in memory, which will grow over time. You don't want to allow uncontrolled growth of data in the memory, so in case the maximum number of submitted jobs to the pipeline is not somehow naturally limited in the system, you have to consider how to do this. In this situation you can, depending on your problem, either: choose that consumer should drop excessive data (using state variable as consumer input instead of flow variable); apply some \"back-pressure\" mechanism that will block producer from sending more data until there is new capacity for consumption; or in case you need all data, you don't want to slow producer by applying back-pressure, and data cannot be fitted in the memory, you can let either producer or consumer store it in large enough memory (database/disk). For example producer can be storing it in memory, and it can pass just file/memory pointers to the consumer as job specifications. One approach to apply back pressure is using the idea of credit . The Producer has credit of say 10 with a given consumer. Every time it outputs a job for a consumer reduces that credit by one. Every time a consumer finishes a job it returns +1 credit on the flow variable of the producer. If it produces positive credit with a given consumer it can send a job, otherwise it must wait to collect all received credits from it's own flow input receiving credits back from the consumer. Node outputs \u00b6 Outputs of the nodes serve many purposes: triggering events, passing data, saving results... They are defined simply as Python C++ outputs . define ( \"outside_temperature\" , units = \"C\" , description = \"measured with I2C sensor\" , retention = 2 ) \\\\ TO - DO Key thing to consider when defining an output is retention : do we need this value just during run-time as for use by other nodes, to keep it temporarily while we keep an eye on experiment execution, or want to keep it forever as the result in the experiment record? You specify this by setting corresponding retention value: Runtime retention \u00b6 retention=0 means that this output is available only to other nodes during running of the workflow. Temporary retention \u00b6 retention=1 in addition to making output available for potential use by other nodes, it also stores it temporarily in the runtime database. Output values in the runtime database can be shown for example in dashboard. They are saved until the next job is run on the runtime. This is useful for values one wants to keep an eye on durig the execution of the workflow, but does not need to be saved forever as experimental record. Forever retention \u00b6 retention=2 in addition to making value available to other nodes and in the runtime database, this outputs are saved forever in the final output datafile. This is useful for results of the experiment, and anything else that should be saved in a persistant record. Setting the output \u00b6 You set the output value with Python C++ outputs . set ( outside_temperature = 25.2 ) \\\\ TO - DO Just as with inputs, the outputs can be int , float , string or anything else that is correct JSON dictionary. All outputs can be set as many times as you wish. For retention value 1-2 , they will be visible as time-series in the database and as an array in the hdf5 file. Testing individual nodes \u00b6 Key requirements when building complex, expensive setups is the ability to develop and test nodes independently on their own ( modularity requirement ), before immersing them in the more complex workflow. This is easily done by using dry-run data supplied to the inputs of the nodes before the main node state loop. This data is used only if the node is running outside of the workflow, and based on this test data, data types for inputs are also determined ( int , float , string or JSON ). In the current implementation, successive setting of values to flow input will queue them for consumption by the node. In current implementation, for state input, only the last set value is taken into account. Testing of state sequences is in development Arriving soon! For example, to execute example Python code from the top of this page, provided it is saved as example.py it is enough to run python example.py Once the .get(...) calls the flow variable for which the queue has been emptied, node will automatically exit. Alternatively, one can terminate execution of the node by pressing Ctrl + C on the keyboard. A node can also call the nodeio.terminate_workflow() method that will request termination of workflow. When a node is running outside the workflow, only that node will terminate. However, if the node is running as a part of the workflow, then all workflow will be terminated. Usually, only one node calls termination of the workflow in complex workflows. During the execution of the node like this, entropynodes folder in the project folder will be automatically populated with information necessary for smooth building of workflow later. Advanced examples & patterns \u00b6 Place for useful patterns/templates. Processing pipeline \u00b6 Sensor readout update \u00b6 Job task-list coordinator \u00b6 Clerk agent \u00b6 checks if there are newly queued jobs. If not, continue with other things. If there are, it can fetch them","title":"NodeIO"},{"location":"entropyhub_flame/nodeio/overview/#nodeio","text":"NodeIO wraps your code \u2014 that provides a solution using some experimental hardware or does some calculations \u2014 written in almost any language. We call that solution a node , and NodeIO provides a universal interface to your solution. Taking NodeIO in the flavour of your language of choice, you can individually test your solution (modularity) before immersing it in a bigger workflow , for which NodeIO provided an interface without any additional work from your side. Crucially, through inputs and outputs provided by NodeIO your solution can trigger, pass and react to events, data and states in other nodes in the workflow. Thanks to NodeIO you explain all of these in a way that closely follows how we humans think, as a story that happens. Note When using EntropyHub, code for nodes and workflows can be edited in Experiments panel with Code view turned on.","title":"NodeIO"},{"location":"entropyhub_flame/nodeio/overview/#example-of-node-definition","text":"Before looking in details, let's see an example: Python QUA C++ LabView # ==================== DEFINE NODE ==================== import entropylab.flame.nodeio as nodeio nodeio . context ( name = \"HelloWorld\" , description = \"example node that concatinates strings\" , icon = \"bootstrap/gear-fill.svg\" ) inputs = nodeio . Inputs () inputs . flow ( \"new_value\" , units = \"\" , description = \"what to add to cummulative sum\" ) outputs = nodeio . Outputs () outputs . define ( \"cumulative_sum\" , units = \"\" , description = \"string after concatination\" , retention = 0 ) nodeio . register () # ==================== DRY RUN DATA ==================== inputs . set ( new_value = \"Hello\" ) inputs . set ( new_value = \" world\" ) inputs . set ( new_value = \"!\" ) # =============== RUN NODE STATE MACHINE =============== current_string = \"\" while nodeio . status . active : outputs . set ( cumulative_sum = current_string ) print ( current_string ) increment = inputs . get ( \"new_value\" ) current_string += increment # TO-DO #include <stdio.h> int main ( void ) { printf ( \"TO-DO! \\n \" ); return 0 ; } TO-DO We see the code has in general three parts: 1. node and I/O definition, 2. dry-run data and 3. user-defined logic that works within a state-machine loop. Looking closely to example you see that: First we use the nodeio library to define node name , used for creating instances of this node when building workflows later. Also, we specify description of the node, and select an icon for visualisation in the workflow view. After that we grab an instance of nodeio.Inputs() and use it to define names of inputs , specifying optional units and some useful description. Here we use the so-called flow variable that we will discuss below. Think about them as a water pipe, they queue all received inputs for the node to be consumed in sequential manner. Then we grab an instance of nodeio.Outputs() and use it to define names of outputs . Additional key-word argument retention will specify now if we need this output just temporarily, as run-time variable only, or we also want to save it forever, as a result. Importantly, at the end of this segment we call nodeio.register() which will register this node as something we can use later on in building workflows. The second segment defines so-called dry-run data . This is used for specifying example inputs of the node, and allows testing and debugging of the node during development of the node, independent of any workflows. It is also used to specify expected data-types for the inputs, which in case of Python is done implicitly. In this specific example, we set the flow variable 3 times. Since this is a flow variable, the input.get function called later for this variable will return these three different values in sequence they were passed to it. Finally, in the third, and the last part, we have user-defined logic for solving specific problems. Importantly, after any initial variable initialisation, all the logic is happening within state-machine loop defined by nodeio.status.active . One breaks out of the loop pressing Ctrl + C , or the program will on its own leave the loop if some of the requested flow variables is empty. During the running, execution relative to other nodes is controlled by setting outputs and getting inputs. For example, If a node is run as a part of a workflow, input.get() will block on flow variables until value is received from some other node. If we run above code in terminal the output will be Hello Hello world Hello world! When running node like this, outside a workflow (dry-run), not only that we check the functioning of the node, but also on the background the necessary node information is added in the automatically-generated folder entropynodes that can be used later on to construct workflows. What data types we can pass as inputs/outputs? To keep this interoperability between different programing languages, node inputs and outputs can be a int , float , string or generic json . JSON can be used to pass any other datatype by serializing it beforehand. We will now look in detail at the options and functionality of nodio inputs and outputs.","title":"Example of node definition"},{"location":"entropyhub_flame/nodeio/overview/#node-inputs","text":"Communication between nodes is asynchronous, and different types of input variables handle differently updates to the inputs. Choosing appropriate input can help you easily write solutions for real-time control with many nodes. With each node input type we provide some typical use cases to help you with selection.","title":"Node inputs"},{"location":"entropyhub_flame/nodeio/overview/#state","text":"A state input saves only the last value passed to it. Requesting multiple times the value of state input always returns the latest existing value, no metter how many times the new value has been set since the last get(...) call. The get(...) call blocks (waits) for value only if the value has not yet been set. A state input It is defined with Python C++ inputs . state ( \"temperature\" , units = \"C\" , description = \"sensor reading of outside temperature\" ) \\\\ TO - DO Value can be obtained with Python C++ value = inputs . get ( \"temperature\" ) \\\\ TO - DO And this request will return the latest set value, unless the value has not been set yet, in which case it will block (wait) until it is set. We can use method updated(...) to check if the value has been updated since the last time we have read it Python C++ if inputs . updated ( \"temperature\" ): print ( \"temperature value has been set again on input\" ) else : print ( \"no updates received\" ) \\\\ TO - DO This always returns immediately. Note that updated returns True even if the same value has been set again (it does not compare with previous value). Some use cases are Providing latest real-time parameters Pipelines for real-time data Imagine you have some process that continuously does analysis of available data about your system and updates some value estimate. The value might be describing best current treshold for discriminating between two states for example. Some other part of your system has to use this knowledge. That part of the system does not care how many updates have been made to the parameter in the meantime by . All wants to do is to use the latest/best available value, while working independently of the . Connecting output as state input for is a perfect solution. Imagine you have a camera taking images of something at 30 fps. This is images are used for several different purposes in the system. Now you want to add to this a special image processing module . However due to heavy processing, your can handle just 20 fps on average. To make best use of the real-time system, you would like to feed to the rest of the system the most up-to-date analysis from the module, so you might need to periodically drop some of the frames. For this connecting output to state input of is a perfect solution.","title":"\r\n\r\n\r\n State"},{"location":"entropyhub_flame/nodeio/overview/#flow","text":"A flow input queues all received values for sequential consumption one by one. Each .get(...) then returns (consumes) one value from the queue. In case the queue is empty, get(...) queue will block (wait). Python C++ inputs . flow ( \"movement_detected\" , units = \"my units for movement detection\" , description = \"details of detected movement\" ) \\\\ TO - DO Value can be obtained same as with any other input Python C++ value = inputs . get ( \"movement_detected\" ) \\\\ TO - DO If the queue is empty this will block (wait) until something sets the input. Repeated calls to get(...) on flow variables return in sequence items from the queue in the order they are received (queue is consumed in order). Sometimes one might just want to check if the new value on the flow input is provided, without blocking the get(...) call. That is provided by updated(...) method Python C++ if inputs . updated ( \"movement_detected\" ): print ( \"New movement has been detected. Details of the movement:\" ) details = inputs . get ( \"movement_detected\" ) print ( details ) else : print ( \"no movement detected\" ) \\\\ TO - DO This always returns immediately, returning True only if there is a value in the flow input queue ready to be consumed with get(...) . Some use cases are Event triggering Data pipeline/queue Imagine you want to trigger e.g. camera on movement . You can solve this by having a camera node that will initialize a camera and wait on flow input channel, connected to node that does motion detector analysis maybe based on some sensors, to trigger an event . Imagine you have a chain of data analysis nodes ( , , , , ...) operating in sequence on a given data. You can pass these data tasks from one node to another using flow inputs. . Note in case that you data producers are faster than data consumers, the data buffer will be growing, occupying memory. If data production is not naturally limited within the system, unlimited amounts of data can be produced over time, while the consumer is limited in ingestion. Then the buffer would request ever-growing memory in this case. This unlimited memory cannot be provided of course. To handle such cases please check the note below. Note: what to do when producer is faster than consumer Note that simple flow input does not provide back-pressure on producers. That is, if producer is producing output at rate larger than the input node consumption on flow input, the data will be saved in memory, which will grow over time. You don't want to allow uncontrolled growth of data in the memory, so in case the maximum number of submitted jobs to the pipeline is not somehow naturally limited in the system, you have to consider how to do this. In this situation you can, depending on your problem, either: choose that consumer should drop excessive data (using state variable as consumer input instead of flow variable); apply some \"back-pressure\" mechanism that will block producer from sending more data until there is new capacity for consumption; or in case you need all data, you don't want to slow producer by applying back-pressure, and data cannot be fitted in the memory, you can let either producer or consumer store it in large enough memory (database/disk). For example producer can be storing it in memory, and it can pass just file/memory pointers to the consumer as job specifications. One approach to apply back pressure is using the idea of credit . The Producer has credit of say 10 with a given consumer. Every time it outputs a job for a consumer reduces that credit by one. Every time a consumer finishes a job it returns +1 credit on the flow variable of the producer. If it produces positive credit with a given consumer it can send a job, otherwise it must wait to collect all received credits from it's own flow input receiving credits back from the consumer.","title":"\r\n\r\n\r\n\r\n\r\n Flow"},{"location":"entropyhub_flame/nodeio/overview/#node-outputs","text":"Outputs of the nodes serve many purposes: triggering events, passing data, saving results... They are defined simply as Python C++ outputs . define ( \"outside_temperature\" , units = \"C\" , description = \"measured with I2C sensor\" , retention = 2 ) \\\\ TO - DO Key thing to consider when defining an output is retention : do we need this value just during run-time as for use by other nodes, to keep it temporarily while we keep an eye on experiment execution, or want to keep it forever as the result in the experiment record? You specify this by setting corresponding retention value:","title":"Node outputs"},{"location":"entropyhub_flame/nodeio/overview/#runtime-retention","text":"retention=0 means that this output is available only to other nodes during running of the workflow.","title":"\r\n\r\n  \r\n  \r\n  \r\n  \r\n Runtime retention"},{"location":"entropyhub_flame/nodeio/overview/#temporary-retention","text":"retention=1 in addition to making output available for potential use by other nodes, it also stores it temporarily in the runtime database. Output values in the runtime database can be shown for example in dashboard. They are saved until the next job is run on the runtime. This is useful for values one wants to keep an eye on durig the execution of the workflow, but does not need to be saved forever as experimental record.","title":"\r\n\r\n  \r\n  \r\n  \r\n  \r\n  \r\n Temporary retention"},{"location":"entropyhub_flame/nodeio/overview/#forever-retention","text":"retention=2 in addition to making value available to other nodes and in the runtime database, this outputs are saved forever in the final output datafile. This is useful for results of the experiment, and anything else that should be saved in a persistant record.","title":"\r\n\r\n  \r\n  \r\n  \r\n  \r\n Forever retention"},{"location":"entropyhub_flame/nodeio/overview/#setting-the-output","text":"You set the output value with Python C++ outputs . set ( outside_temperature = 25.2 ) \\\\ TO - DO Just as with inputs, the outputs can be int , float , string or anything else that is correct JSON dictionary. All outputs can be set as many times as you wish. For retention value 1-2 , they will be visible as time-series in the database and as an array in the hdf5 file.","title":"Setting the output"},{"location":"entropyhub_flame/nodeio/overview/#testing-individual-nodes","text":"Key requirements when building complex, expensive setups is the ability to develop and test nodes independently on their own ( modularity requirement ), before immersing them in the more complex workflow. This is easily done by using dry-run data supplied to the inputs of the nodes before the main node state loop. This data is used only if the node is running outside of the workflow, and based on this test data, data types for inputs are also determined ( int , float , string or JSON ). In the current implementation, successive setting of values to flow input will queue them for consumption by the node. In current implementation, for state input, only the last set value is taken into account. Testing of state sequences is in development Arriving soon! For example, to execute example Python code from the top of this page, provided it is saved as example.py it is enough to run python example.py Once the .get(...) calls the flow variable for which the queue has been emptied, node will automatically exit. Alternatively, one can terminate execution of the node by pressing Ctrl + C on the keyboard. A node can also call the nodeio.terminate_workflow() method that will request termination of workflow. When a node is running outside the workflow, only that node will terminate. However, if the node is running as a part of the workflow, then all workflow will be terminated. Usually, only one node calls termination of the workflow in complex workflows. During the execution of the node like this, entropynodes folder in the project folder will be automatically populated with information necessary for smooth building of workflow later.","title":"Testing individual nodes"},{"location":"entropyhub_flame/nodeio/overview/#advanced-examples-patterns","text":"Place for useful patterns/templates.","title":"Advanced examples &amp; patterns"},{"location":"entropyhub_flame/nodeio/overview/#processing-pipeline","text":"","title":"Processing pipeline"},{"location":"entropyhub_flame/nodeio/overview/#sensor-readout-update","text":"","title":"Sensor readout update"},{"location":"entropyhub_flame/nodeio/overview/#job-task-list-coordinator","text":"","title":"Job task-list coordinator"},{"location":"entropyhub_flame/nodeio/overview/#clerk-agent","text":"checks if there are newly queued jobs. If not, continue with other things. If there are, it can fetch them","title":"Clerk agent"},{"location":"entropyhub_flame/observability/overview/","text":"Observability \u00b6 Entropy Hub gives you observability on node level (for tracking and debugging individual parts of the system), workflow level (for tracking and debugging system-level issues), and also on the level of the results of the individual nodes, both through real-time dashboard for result tracking during the execution, and the previous experimental record. Finally, you can track the status of job queues, where each job consists of the workflow and parameters. Workflow status \u00b6 Workflow graph view with push notifications... Real-time outputs dashboard \u00b6 Real-time, human-in-the-loop, tracking of progress of the experiment through node outputs can be done from Dashboard panel in Entropy-hub menu that by default opens Grafana plug-in. Note that only node outputs with retention 1 or 2 will be visible ( check nodeio retention rules here ). You can save pre-configured dashboard easily once you create them. Each dashboard consists of one or several panels, that use different visualisation plugins. You can also setup alarms. Custom plot generated by your favourite plotting library \u00b6 If you have a custom plotting as a part of your workflow, you can show this in the dashboard simply if you export the plot as base64 encoded binary image. In example below we export custom matplotlib plot as .png as a part of the JSON dictionary output under png field. # nodeio code preceeds... import numpy as np import matplotlib.pyplot as plt import io import base64 while nodeio . status . active : a = input . get ( \"data\" ) # some code ... # do some custom plot x = np . linspace ( 0 , 10 , 100 ) y = np . cos ( a * x / 10 ) plt . figure () plt . plot ( x , y , \"b-\" ) plt . plot ( x , np . cos ( y ) / 3 , \"r-\" ) plt . xlabel ( \"Time (s)\" ) plt . ylabel ( \"Amplitude (rel.un)\" ) plt . legend ([ \"probe1\" , \"drive1\" ]) buffer = io . BytesIO () plt . savefig ( buffer , format = \"png\" ) buffer . seek ( 0 ) figure_base64_encoded_png = base64 . b64encode ( buffer . read ()) . decode () output . set ( final_plot = { \"png\" : figure_base64_encoded_png }) You can get that plot in Dashboard by (1) selecting that node output as a data source and (2) using value->>'png' query (note double >> in the query) to obtain directly base64 encoded plot (switch to Table view to verify this). (3) This can be then shown using Base 64 Image/PDF plugin Info Check PostgreSQL JSON operators for further options when quering JSON. Generate plot of triggered waveform \u00b6 If you have a triggered scope, that delivers to you trace readout after every trigger, and you want to visualise this data in the dashboard. For example your node output is # nodeio code preceeds... import numpy as np while nodeio . status . active : a = input . get ( \"data\" ) # simulate osciloscope data input x = np . linspace ( 0 , 10 , 100 ) y = np . cos ( a * x / 10 ) output . set ( estimated_parameter = y . tolist ()) You can select that data and use the Plotly panel option, with, as an example, the following code under Script option for Plotly to obtain latest output and regenerate time-axis with times step dt=0.1 seconds. Script code (4. on the image above): // console.log(data) var last_trigger = data . series [ 0 ]. fields [ 1 ]. values . buffer . length - 1 ; // get y-axis data var y = JSON . parse ( data . series [ 0 ]. fields [ 1 ]. values . buffer [ last_trigger ]); // generate x-axis // time of the record var t = data . series [ 0 ]. fields [ 0 ]. values . buffer [ last_trigger ]; dt = 1.0 ; // scope time set var x = []; for ( var i = 0 ; i < y . length ; i ++ ) x . push ( t + i * dt ); var trace = { x : x , y : y , }; return { data : [ trace ], layout : { title : \"\" } }; Info Many more advanced plotting styles and options are supported through standard Plotly.js methods. Check panel documentation as well as Plotly.js documentation . Job queues and status \u00b6 Past jobs and job status can be seen by clicking on the burger icon from the Experiments panel top left corner. The job list that opens is grouped by workflows. Clicking on a particular workflow expands that workflow and shows jobs that run with that workflow.","title":"Observability"},{"location":"entropyhub_flame/observability/overview/#observability","text":"Entropy Hub gives you observability on node level (for tracking and debugging individual parts of the system), workflow level (for tracking and debugging system-level issues), and also on the level of the results of the individual nodes, both through real-time dashboard for result tracking during the execution, and the previous experimental record. Finally, you can track the status of job queues, where each job consists of the workflow and parameters.","title":"Observability"},{"location":"entropyhub_flame/observability/overview/#workflow-status","text":"Workflow graph view with push notifications...","title":"Workflow status"},{"location":"entropyhub_flame/observability/overview/#real-time-outputs-dashboard","text":"Real-time, human-in-the-loop, tracking of progress of the experiment through node outputs can be done from Dashboard panel in Entropy-hub menu that by default opens Grafana plug-in. Note that only node outputs with retention 1 or 2 will be visible ( check nodeio retention rules here ). You can save pre-configured dashboard easily once you create them. Each dashboard consists of one or several panels, that use different visualisation plugins. You can also setup alarms.","title":"Real-time outputs dashboard"},{"location":"entropyhub_flame/observability/overview/#custom-plot-generated-by-your-favourite-plotting-library","text":"If you have a custom plotting as a part of your workflow, you can show this in the dashboard simply if you export the plot as base64 encoded binary image. In example below we export custom matplotlib plot as .png as a part of the JSON dictionary output under png field. # nodeio code preceeds... import numpy as np import matplotlib.pyplot as plt import io import base64 while nodeio . status . active : a = input . get ( \"data\" ) # some code ... # do some custom plot x = np . linspace ( 0 , 10 , 100 ) y = np . cos ( a * x / 10 ) plt . figure () plt . plot ( x , y , \"b-\" ) plt . plot ( x , np . cos ( y ) / 3 , \"r-\" ) plt . xlabel ( \"Time (s)\" ) plt . ylabel ( \"Amplitude (rel.un)\" ) plt . legend ([ \"probe1\" , \"drive1\" ]) buffer = io . BytesIO () plt . savefig ( buffer , format = \"png\" ) buffer . seek ( 0 ) figure_base64_encoded_png = base64 . b64encode ( buffer . read ()) . decode () output . set ( final_plot = { \"png\" : figure_base64_encoded_png }) You can get that plot in Dashboard by (1) selecting that node output as a data source and (2) using value->>'png' query (note double >> in the query) to obtain directly base64 encoded plot (switch to Table view to verify this). (3) This can be then shown using Base 64 Image/PDF plugin Info Check PostgreSQL JSON operators for further options when quering JSON.","title":"Custom plot generated by your favourite plotting library"},{"location":"entropyhub_flame/observability/overview/#generate-plot-of-triggered-waveform","text":"If you have a triggered scope, that delivers to you trace readout after every trigger, and you want to visualise this data in the dashboard. For example your node output is # nodeio code preceeds... import numpy as np while nodeio . status . active : a = input . get ( \"data\" ) # simulate osciloscope data input x = np . linspace ( 0 , 10 , 100 ) y = np . cos ( a * x / 10 ) output . set ( estimated_parameter = y . tolist ()) You can select that data and use the Plotly panel option, with, as an example, the following code under Script option for Plotly to obtain latest output and regenerate time-axis with times step dt=0.1 seconds. Script code (4. on the image above): // console.log(data) var last_trigger = data . series [ 0 ]. fields [ 1 ]. values . buffer . length - 1 ; // get y-axis data var y = JSON . parse ( data . series [ 0 ]. fields [ 1 ]. values . buffer [ last_trigger ]); // generate x-axis // time of the record var t = data . series [ 0 ]. fields [ 0 ]. values . buffer [ last_trigger ]; dt = 1.0 ; // scope time set var x = []; for ( var i = 0 ; i < y . length ; i ++ ) x . push ( t + i * dt ); var trace = { x : x , y : y , }; return { data : [ trace ], layout : { title : \"\" } }; Info Many more advanced plotting styles and options are supported through standard Plotly.js methods. Check panel documentation as well as Plotly.js documentation .","title":"Generate plot of triggered waveform"},{"location":"entropyhub_flame/observability/overview/#job-queues-and-status","text":"Past jobs and job status can be seen by clicking on the burger icon from the Experiments panel top left corner. The job list that opens is grouped by workflows. Clicking on a particular workflow expands that workflow and shows jobs that run with that workflow.","title":"Job queues and status"},{"location":"entropyhub_flame/workflow/overview/","text":"Workflow \u00b6 Workflow allows you to write an experimental workflow as a story in a code that reads like a prose. You can keep thinking at high level how you want to orchestrate your experiment with everything you need \u2014 all documentation of different modules, and inputs and outputs specifications \u2014 available on demand thanks to usual auto-completion features provided by IDE. No matter what is your programming languages of choice for writing nodes, you have a single place to control them all, as a code. You can use as little or as much Python as you want in authoring the experiment, keeping it to bare minimum that is almost only consisting of problem-specific names defined by you, or use full-blown Python for efficient programmatic generation of large complex workflows. Note When using Entropy Hub , code for nodes and workflows can be edited in Experiments panel with Code view turned on. Defining workflow \u00b6 Workflow is specified as a Python file that in general looks like from entropylab.flame.workflow import Workflow import entropynodes.library as expNodes # 1. Define workflow wf = Workflow ( \"A day in office\" , description = \"What happens in local office\" ) # 2. START workflow boss = expNodes . GrumpyAdministrator ( \"boss\" ) clerk = expNodes . CheerfulNode ( \"clerk\" , customer = boss . o . clerk_request ) boss . i . clerk_salary = clerk . o . requested_salary wf . register () # 3. END and register workflow To define workflow we import Workflow from entropylab.workflow and define its name and longer description. All the processing nodes, that we defined using NodeIO and run them individually, are available as entropynodes.library . In the example above we import those as expNodes . We then proceed by describing workflow. We make instances of processing nodes, give them unique name that will help us later track them in the workflow view, and connect their inputs and outputs. Note that there can be many instances of a node defined under expNode . Each instance will be running in parallel, allowing parallel processing. Inputs and outputs of the nodes are listed under .i and .o parameters respectively. If you use IDE with Pylance like Visual Studio, provided integrated in Entropy Hub, you will be able to see all possible inputs and outputs as well as their description and units. Inputs of the nodes can be set during creation of the instance: e.g. customer=a.o.clerk_request sets customer input of node named clerk to output of boss node). Alternatively, they can be set later on as we do when we pass a clerk's request for salary to the boss boss.i.clerk_salary = clerk.o.requested_salary . Not all possible node inputs have to be specified here: unresolved ones become parameters to be provided for the specific run of this workflow, as we describe in the following. Parameters and run-time variables \u00b6 Nodes that make up the workflow have inputs: Some of the inputs might be resolved during the running of the workflow, by connecting outputs of other nodes: these are run-time variables . Other inputs will be specified by the user before running of the workflow: these are parameters . Once workflow is defined, user can execute in command line workflow.py file that specifies workflow python3 workflow.py to automatically generate a list of all parameters of the workflow as a simple JSON file parameters.json . This is a list of unresolved input values that have to be specified before executing this workflow can be executed. Note If there is existing parameters.json file that already specifies some of the values of the workflow, running python workflow.py will overwrite this file, but, it will try to reuse any values that were set in the old file, and write them also in the new file (as long as there is a match between some of the old and new node names and their variable names). Executing a workflow \u00b6 From command line \u00b6 python3 -m entropylab.flame.execute in general usage: execute.py [-h] [-w WORKFLOW] [-p PARAMETERS] [-t MAX_EXECUTION_TIME] [-d STATUS_CHECK_INTERVAL] Flame executor of parametrized workflows optional arguments: -h, --help show this help message and exit -w WORKFLOW, --workflow WORKFLOW Python file that defines main entropylab.Workflow (default workflow.py) -p PARAMETERS, --parameters PARAMETERS JSON file that resolves workflow parameters -t MAX_EXECUTION_TIME, --max-execution-time MAX_EXECUTION_TIME Maximal execution time in s -d STATUS_CHECK_INTERVAL, --status-check-interval STATUS_CHECK_INTERVAL Node status check interval in s (default 1 s) From user interface \u00b6 There are two possible ways to submit a job (=workflow + parameters) for execution: Experiment > workflow view > Run Experiment > Job list > Add Advanced topics \u00b6 Grouping nodes into different views \u00b6 Importing workflows into larger workflows \u00b6 Distributed workflows spanning multiple hosts \u00b6","title":"Workflow"},{"location":"entropyhub_flame/workflow/overview/#workflow","text":"Workflow allows you to write an experimental workflow as a story in a code that reads like a prose. You can keep thinking at high level how you want to orchestrate your experiment with everything you need \u2014 all documentation of different modules, and inputs and outputs specifications \u2014 available on demand thanks to usual auto-completion features provided by IDE. No matter what is your programming languages of choice for writing nodes, you have a single place to control them all, as a code. You can use as little or as much Python as you want in authoring the experiment, keeping it to bare minimum that is almost only consisting of problem-specific names defined by you, or use full-blown Python for efficient programmatic generation of large complex workflows. Note When using Entropy Hub , code for nodes and workflows can be edited in Experiments panel with Code view turned on.","title":"Workflow"},{"location":"entropyhub_flame/workflow/overview/#defining-workflow","text":"Workflow is specified as a Python file that in general looks like from entropylab.flame.workflow import Workflow import entropynodes.library as expNodes # 1. Define workflow wf = Workflow ( \"A day in office\" , description = \"What happens in local office\" ) # 2. START workflow boss = expNodes . GrumpyAdministrator ( \"boss\" ) clerk = expNodes . CheerfulNode ( \"clerk\" , customer = boss . o . clerk_request ) boss . i . clerk_salary = clerk . o . requested_salary wf . register () # 3. END and register workflow To define workflow we import Workflow from entropylab.workflow and define its name and longer description. All the processing nodes, that we defined using NodeIO and run them individually, are available as entropynodes.library . In the example above we import those as expNodes . We then proceed by describing workflow. We make instances of processing nodes, give them unique name that will help us later track them in the workflow view, and connect their inputs and outputs. Note that there can be many instances of a node defined under expNode . Each instance will be running in parallel, allowing parallel processing. Inputs and outputs of the nodes are listed under .i and .o parameters respectively. If you use IDE with Pylance like Visual Studio, provided integrated in Entropy Hub, you will be able to see all possible inputs and outputs as well as their description and units. Inputs of the nodes can be set during creation of the instance: e.g. customer=a.o.clerk_request sets customer input of node named clerk to output of boss node). Alternatively, they can be set later on as we do when we pass a clerk's request for salary to the boss boss.i.clerk_salary = clerk.o.requested_salary . Not all possible node inputs have to be specified here: unresolved ones become parameters to be provided for the specific run of this workflow, as we describe in the following.","title":"Defining workflow"},{"location":"entropyhub_flame/workflow/overview/#parameters-and-run-time-variables","text":"Nodes that make up the workflow have inputs: Some of the inputs might be resolved during the running of the workflow, by connecting outputs of other nodes: these are run-time variables . Other inputs will be specified by the user before running of the workflow: these are parameters . Once workflow is defined, user can execute in command line workflow.py file that specifies workflow python3 workflow.py to automatically generate a list of all parameters of the workflow as a simple JSON file parameters.json . This is a list of unresolved input values that have to be specified before executing this workflow can be executed. Note If there is existing parameters.json file that already specifies some of the values of the workflow, running python workflow.py will overwrite this file, but, it will try to reuse any values that were set in the old file, and write them also in the new file (as long as there is a match between some of the old and new node names and their variable names).","title":"Parameters and run-time variables"},{"location":"entropyhub_flame/workflow/overview/#executing-a-workflow","text":"","title":"Executing a workflow"},{"location":"entropyhub_flame/workflow/overview/#from-command-line","text":"python3 -m entropylab.flame.execute in general usage: execute.py [-h] [-w WORKFLOW] [-p PARAMETERS] [-t MAX_EXECUTION_TIME] [-d STATUS_CHECK_INTERVAL] Flame executor of parametrized workflows optional arguments: -h, --help show this help message and exit -w WORKFLOW, --workflow WORKFLOW Python file that defines main entropylab.Workflow (default workflow.py) -p PARAMETERS, --parameters PARAMETERS JSON file that resolves workflow parameters -t MAX_EXECUTION_TIME, --max-execution-time MAX_EXECUTION_TIME Maximal execution time in s -d STATUS_CHECK_INTERVAL, --status-check-interval STATUS_CHECK_INTERVAL Node status check interval in s (default 1 s)","title":"From command line"},{"location":"entropyhub_flame/workflow/overview/#from-user-interface","text":"There are two possible ways to submit a job (=workflow + parameters) for execution: Experiment > workflow view > Run Experiment > Job list > Add","title":"From user interface"},{"location":"entropyhub_flame/workflow/overview/#advanced-topics","text":"","title":"Advanced topics"},{"location":"entropyhub_flame/workflow/overview/#grouping-nodes-into-different-views","text":"","title":"Grouping nodes into different views"},{"location":"entropyhub_flame/workflow/overview/#importing-workflows-into-larger-workflows","text":"","title":"Importing workflows into larger workflows"},{"location":"entropyhub_flame/workflow/overview/#distributed-workflows-spanning-multiple-hosts","text":"","title":"Distributed workflows spanning multiple hosts"},{"location":"paramstore/overview/","text":"Parameter Store \u00b6 Used to save parameters","title":"ParameterStore"},{"location":"paramstore/overview/#parameter-store","text":"Used to save parameters","title":"Parameter Store"},{"location":"pipeline/getting_started/","text":"Getting started \u00b6 Entropy pipeline is a python package for lab workflow management. It is built for streamlining running quantum information processing experiments. Entropy's goal is to solve a few major hurdles in experiment design: Building, maintaining and executing complex experiments Data collection Device management Calibration automation To tackle these problems, Entropy revolves one central structure: an execution graph. The nodes of a graph give us a convenient way to brake down experiments into stages and to automate some repetitive tasks. For example data collection is automated, at least in part, by saving node data and code to a database. Device management is the challenge of managing the state and control of a variety of different resources. These include, but are not limited to, lab instrumnets. They can also be computational resources, software resources or others. Entropy is built with tools to save such resources to a shared database and give nodes access to the resources needed during an experiment. Performing automatic calibration is an important reason why we built Entropy. This could be though of as the usecase most clearly benefiting from shared resources, persistant storage of different pieced of information and the graph structure. If the final node in a graph is the target experiment, then all the nodes between the root and that node are often calibration steps. The documentation section will show how this can be done. The Entropy system is built with concrete implemnetations of the various parts (database backend, resource managment and others) but is meant to be completely customizable. Any or every part of the system can be tailored by end users. Versioning and the Alpha release \u00b6 The current release of Entropy is version 0.1.0. You can learn more about the Entropy versioning scheme in the versioning document. This means this version is a work in progress in several important ways: It is not fully tested There are important features missing, such as the results GUI which will enable visual results viewing and automatic plotting There will more than likely be breaking changes to the API for a while until we learn how things should be done. Keep this in mind as you start your journey. Installation \u00b6 Installation is done from pypi using the following command pip install entropylab Testing your installation \u00b6 import the library from entropylab from entropylab import * def my_func (): return { 'res' : 1 } node1 = PyNode ( \"first_node\" , my_func , output_vars = { 'res' }) experiment = Graph ( None , { node1 }, \"run_a\" ) # No resources used here handle = experiment . run () Usage \u00b6 See docs folder in this repository for all the dirty details. Extensions \u00b6 Entropy can and will be extended via custom extensions. An example is entropylab-qpudb , an extension built to keep track of the calibration parameters of a mutli-qubit Quantum Processing Unit (QPU). This extension is useful when writing an automatic calibration graph.","title":"Getting started"},{"location":"pipeline/getting_started/#getting-started","text":"Entropy pipeline is a python package for lab workflow management. It is built for streamlining running quantum information processing experiments. Entropy's goal is to solve a few major hurdles in experiment design: Building, maintaining and executing complex experiments Data collection Device management Calibration automation To tackle these problems, Entropy revolves one central structure: an execution graph. The nodes of a graph give us a convenient way to brake down experiments into stages and to automate some repetitive tasks. For example data collection is automated, at least in part, by saving node data and code to a database. Device management is the challenge of managing the state and control of a variety of different resources. These include, but are not limited to, lab instrumnets. They can also be computational resources, software resources or others. Entropy is built with tools to save such resources to a shared database and give nodes access to the resources needed during an experiment. Performing automatic calibration is an important reason why we built Entropy. This could be though of as the usecase most clearly benefiting from shared resources, persistant storage of different pieced of information and the graph structure. If the final node in a graph is the target experiment, then all the nodes between the root and that node are often calibration steps. The documentation section will show how this can be done. The Entropy system is built with concrete implemnetations of the various parts (database backend, resource managment and others) but is meant to be completely customizable. Any or every part of the system can be tailored by end users.","title":"Getting started"},{"location":"pipeline/getting_started/#versioning-and-the-alpha-release","text":"The current release of Entropy is version 0.1.0. You can learn more about the Entropy versioning scheme in the versioning document. This means this version is a work in progress in several important ways: It is not fully tested There are important features missing, such as the results GUI which will enable visual results viewing and automatic plotting There will more than likely be breaking changes to the API for a while until we learn how things should be done. Keep this in mind as you start your journey.","title":"Versioning and the Alpha release"},{"location":"pipeline/getting_started/#installation","text":"Installation is done from pypi using the following command pip install entropylab","title":"Installation"},{"location":"pipeline/getting_started/#testing-your-installation","text":"import the library from entropylab from entropylab import * def my_func (): return { 'res' : 1 } node1 = PyNode ( \"first_node\" , my_func , output_vars = { 'res' }) experiment = Graph ( None , { node1 }, \"run_a\" ) # No resources used here handle = experiment . run ()","title":"Testing your installation"},{"location":"pipeline/getting_started/#usage","text":"See docs folder in this repository for all the dirty details.","title":"Usage"},{"location":"pipeline/getting_started/#extensions","text":"Entropy can and will be extended via custom extensions. An example is entropylab-qpudb , an extension built to keep track of the calibration parameters of a mutli-qubit Quantum Processing Unit (QPU). This extension is useful when writing an automatic calibration graph.","title":"Extensions"},{"location":"quam/overview/","text":"QuAM - Quantum Abstract Machine \u00b6 It's impossible to describe a quantum computer in a way that's disconnected from the specifics of the hardware implementation. Nevertheless, some of the tasks you need to accomplish Building the software around a quantum computer is Admin \u00b6 User \u00b6 Oracle \u00b6","title":"Overview"},{"location":"quam/overview/#quam-quantum-abstract-machine","text":"It's impossible to describe a quantum computer in a way that's disconnected from the specifics of the hardware implementation. Nevertheless, some of the tasks you need to accomplish Building the software around a quantum computer is","title":"QuAM - Quantum Abstract Machine"},{"location":"quam/overview/#admin","text":"","title":"Admin"},{"location":"quam/overview/#user","text":"","title":"User"},{"location":"quam/overview/#oracle","text":"","title":"Oracle"}]}